{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorectal polyp segmentation #\n",
    "The code below uses the CVC-ClinicDB public data set which consists of 612 images with corresponding segmentation masks. The dataset can be found here: https://www.kaggle.com/balraj98/cvcclinicdb?select=metadata.csv.\n",
    "\n",
    "To run the code, put the original .tif files in the `/data/images` folder and the segmentation masks (ground truth) in the `/data/masks` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from Losses import ComboLoss, dice_metric\n",
    "from Split import StratifiedGroupKFold \n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "\n",
    "print('torch.cuda.is_available():', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_ipython().__class_._name__\n",
    "    from tqdm.notebook import tqdm\n",
    "except:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR         = Path(f'data/')\n",
    "TRAIN_IMG_DIR    = DATA_DIR/'images'\n",
    "TRAIN_MASK_DIR   = DATA_DIR/'masks'\n",
    "RLE_DF_PATH      = 'rle.csv'\n",
    "KFOLD_PATH       = 'rle_kfold.csv'\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 16\n",
    "USE_SAMPLER      = False\n",
    "ENCODER          = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS  = 'imagenet'\n",
    "CLASSES          = ['adenomatous', 'hyperplastic']\n",
    "ACTIVATION       = None \n",
    "DEVICE           = 'cuda'\n",
    "PRETRAINED_PATH  = 'models/bst_model512_fold0_0.7811.bin'\n",
    "PRETRAINED       = False\n",
    "LEARNING_RATE    = 1e-4 #1e-3 or 1e-4 for aggressive training, 1e-5 for normal\n",
    "EPOCHS           = 12\n",
    "LOSS_FN          = 'mixed'\n",
    "CRITERION        = ComboLoss(**{'weights':{'bce':3, 'dice':1, 'focal':4}})\n",
    "USE_CRIT         = True\n",
    "TRAIN_MODEL      = True\n",
    "EVALUATE         = True\n",
    "FOLD_COUNT       = 5\n",
    "FOLD_ID          = 0\n",
    "IMG_SIZE         = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The public datasets have more than 2 colors (due to gradients around the edges). Convert those colors to either black or white\n",
    "def map_binary(mask):\n",
    "    (threshold, black_and_white) = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    return black_and_white\n",
    "\n",
    "\"\"\"\n",
    "black = 0 -> background\n",
    "red = 111 -> adenomatous\n",
    "green = 170 -> hyperplastic\n",
    "white = 255 -> adenomatous\n",
    "\"\"\"\n",
    "\n",
    "# Convert mask colors to the nearest color\n",
    "def nearest_color(mask):\n",
    "    decode_mask = np.zeros((mask.shape[0], mask.shape[1]))\n",
    "    decode_mask[mask < 56] = 0 # Black background\n",
    "    decode_mask[(mask >= 56) & (mask < 141)] = 1 # Own dataset adenomatous polyps\n",
    "    decode_mask[(mask >= 213)] = 1 # Public dataset adenomatous polyps\n",
    "    decode_mask[(mask >= 141) & (mask < 213)] = 2 # Own dataset hyperplastic polyps\n",
    "    \n",
    "    return decode_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    fig,ax = plt.subplots(figsize=(10,6))\n",
    "    ax.imshow(img.permute(1,2,0).numpy())\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    images = {k:v.numpy() for k,v in images.items() if isinstance(v, torch.Tensor)} #convert tensor to numpy \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    image, mask = images['image'], images['mask']\n",
    "    plt.imshow(image.transpose(1,2,0), vmin=0, vmax=1)\n",
    "    if mask.max()>0:\n",
    "        plt.imshow(mask.squeeze(0), alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {}\n",
    "# image_ids = []\n",
    "# image_classes = []\n",
    "\n",
    "# for file in tqdm(list(TRAIN_MASK_DIR.glob('**/*.*'))):\n",
    "#     source = str(file.parent).split(os.sep)[-1]\n",
    "#     image_id = f'{source}/{file.stem}{file.suffix}'\n",
    "#     img = cv2.imread(f'data/masks/{image_id}', 0)\n",
    "#     class_ids = np.unique(nearest_color(img), return_counts=True)\n",
    "\n",
    "#     if (len(class_ids[1]) == 2):\n",
    "#         image_classes.append(1)\n",
    "#     else:\n",
    "#         if (class_ids[1][1] > class_ids[1][2]):\n",
    "#             image_classes.append(1)\n",
    "#         else:\n",
    "#             image_classes.append(2)\n",
    "    \n",
    "#     image_ids.append(image_id)\n",
    "\n",
    "# data['ImageId'] = image_ids\n",
    "# data['ClassId'] = image_classes\n",
    "# rle_df = pd.DataFrame(data, columns=['ImageId', 'ClassId'])\n",
    "# rle_df.to_csv(RLE_DF_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_tmp_df  = pd.read_csv(RLE_DF_PATH, names=['ImageId', 'ClassId'], skiprows=1)\n",
    "cvc_metadata_df  = pd.read_csv('cvc-metadata.csv')\n",
    "etis_metadata_df = pd.read_csv('etis-metadata.csv')\n",
    "uah_metadata_df  = pd.read_csv('uah-metadata.csv').drop(['ClassId'], axis=1)\n",
    "\n",
    "dfs              = [combined_tmp_df, cvc_metadata_df, etis_metadata_df, uah_metadata_df]\n",
    "RLE_DF           = reduce(lambda left, right: pd.merge(left, right, how='outer', on='ImageId'), dfs)\n",
    "\n",
    "RLE_DF['sequence_id_tmp'] = RLE_DF['sequence_id_x'].combine_first(RLE_DF['sequence_id_y'])\n",
    "RLE_DF['sequence'] = RLE_DF['sequence_id'].combine_first(RLE_DF['sequence_id_tmp'])\n",
    "RLE_DF = RLE_DF.drop(columns=['sequence_id_x', 'sequence_id_y', 'sequence_id_tmp', 'sequence_id'])\n",
    "\n",
    "RLE_DF['kfold'] = -1\n",
    "\n",
    "gkf = GroupKFold(n_splits=FOLD_COUNT)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(gkf.split(RLE_DF.ImageId, RLE_DF.ClassId, RLE_DF.sequence)):\n",
    "    RLE_DF.loc[test_index, 'kfold'] = fold\n",
    "    \n",
    "RLE_DF.to_csv(KFOLD_PATH, index=False)\n",
    "\n",
    "RLE_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(KFOLD_PATH)\n",
    "TRAIN_DF = DF.query(f'kfold!={FOLD_ID}').reset_index(drop=True)\n",
    "VAL_DF = DF.query(f'kfold=={FOLD_ID}').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, rle_df, image_base_dir, masks_base_dir, augmentation=None):\n",
    "        self.df             = rle_df\n",
    "        self.image_base_dir = image_base_dir\n",
    "        self.masks_base_dir = masks_base_dir\n",
    "        self.image_ids      = rle_df.ImageId.values\n",
    "        self.augmentation   = augmentation\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_id  = self.image_ids[i]\n",
    "        img_path  = os.path.join(str(self.image_base_dir), str(image_id)) \n",
    "        mask_path = os.path.join(str(self.masks_base_dir), str(image_id))\n",
    "        image     = cv2.imread(img_path, 1)\n",
    "        mask      = nearest_color(cv2.imread(mask_path, 0))     \n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = {\"image\": image, \"mask\": mask}\n",
    "            sample = self.augmentation(**sample)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return {\n",
    "            'image': image, \n",
    "            'mask' : mask\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformations = albu.Compose([\n",
    "    albu.SmallestMaxSize(max_size=288),\n",
    "    albu.CenterCrop(288, 288),\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.Rotate(),\n",
    "    albu.Normalize(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "test_transformations = albu.Compose([\n",
    "    albu.SmallestMaxSize(max_size=288),\n",
    "    albu.CenterCrop(288, 288),\n",
    "    albu.Normalize(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(TRAIN_DF, TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transformations)\n",
    "test_dataset = Dataset(VAL_DF, TRAIN_IMG_DIR, TRAIN_MASK_DIR, test_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = train_dataset[0]['image'], train_dataset[0]['mask']\n",
    "image.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(**train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, TRAIN_BATCH_SIZE, \n",
    "                              shuffle=True if not USE_SAMPLER else False, \n",
    "                              num_workers=0, \n",
    "                              sampler=SAMPLER if USE_SAMPLER else None)\n",
    "val_dataloader   = DataLoader(test_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks = next(iter(train_dataloader))['image'], next(iter(train_dataloader))['mask']\n",
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid = torchvision.utils.make_grid(images[:9], nrow=3, normalize=True)\n",
    "matplotlib_imshow(img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    loss = ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "    print(f'Dice loss: {loss}')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        print(f'Focal loss: {loss.mean()}')\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.focal = FocalLoss(gamma)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n",
    "        print(f'Mixed loss: {loss.mean()}')\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {} out of {}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        model_path = Path(model_path)\n",
    "        parent = model_path.parent\n",
    "        os.makedirs(parent, exist_ok=True)\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\n",
    "                \"Validation score improved ({} --> {}). Model saved at at {}!\".format(\n",
    "                    self.val_score, epoch_score, model_path\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, optimizer, loss_fn, accumulation_steps=1, device='cuda'):\n",
    "    losses = AverageMeter()\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    if accumulation_steps > 1: \n",
    "        optimizer.zero_grad()\n",
    "    tk0 = tqdm(train_loader, total=len(train_loader))\n",
    "    for b_idx, data in enumerate(tk0):\n",
    "        for key, value in data.items():\n",
    "            data[key] = value.to(device)\n",
    "        if accumulation_steps == 1 and b_idx == 0:\n",
    "            optimizer.zero_grad()\n",
    "        image = data['image']\n",
    "        print(image.shape)\n",
    "        print()\n",
    "        out  = model(image)\n",
    "        print(out.shape)\n",
    "        print()\n",
    "        mask = data['mask']\n",
    "        print(mask.shape)\n",
    "        loss = loss_fn(out, mask)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            loss.backward()\n",
    "            if (b_idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        losses.update(loss.item(), train_loader.batch_size)\n",
    "        tk0.set_postfix(loss=losses.avg, learning_rate=optimizer.param_groups[0]['lr'])\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(input, target):\n",
    "    inp = torch.where(input>0.5, torch.tensor(1, device='cuda'), torch.tensor(0, device='cuda'))\n",
    "    acc = (inp.squeeze(1) == target).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_loader, model, device='cuda', metric=dice_metric):\n",
    "    losses = AverageMeter()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    tk0 = tqdm(valid_loader, total=len(valid_loader))\n",
    "    with torch.no_grad():\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(device)\n",
    "            out   = model(data['image'])\n",
    "            out   = torch.sigmoid(out)\n",
    "            dice  = metric(out, data['mask']).cpu()\n",
    "            losses.update(dice.mean().item(), valid_loader.batch_size)\n",
    "            tk0.set_postfix(dice_score=losses.avg)\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED: \n",
    "    model.load_state_dict(torch.load(PRETRAINED_PATH))\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr= LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer, milestones=[3,5,6,7,8,9,10,11,13,15], gamma=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MixedLoss(10.0, 2.0) if not USE_CRIT else CRITERION \n",
    "es = EarlyStopping(patience=10, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss = train_one_epoch(train_dataloader, model, optimizer, criterion)\n",
    "        dice = evaluate(val_dataloader, model, metric=metric)\n",
    "        scheduler.step()\n",
    "        print(f\"EPOCH: {epoch}, TRAIN LOSS: {loss}, VAL DICE: {dice}\")\n",
    "        es(dice, model, model_path=f\"/models/bst_model{IMG_SIZE}_fold{FOLD_ID}_{np.round(dice,4)}.bin\")\n",
    "        best_model = f\"../data/bst_model{IMG_SIZE}__fold{FOLD_ID}_{np.round(es.best_score,4)}.bin\"\n",
    "        if es.early_stop:\n",
    "            print('\\n\\n -------------- EARLY STOPPING -------------- \\n\\n')\n",
    "            break\n",
    "if EVALUATE:\n",
    "    valid_score = evaluate(val_dataloader, model, metric=metric)\n",
    "    print(f\"Valid dice score: {valid_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(img, dim):\n",
    "    width, height = img.shape[1], img.shape[0]\n",
    "\n",
    "    # process crop width and height for max available dimension\n",
    "    crop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
    "    crop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0] \n",
    "    mid_x, mid_y = int(width/2), int(height/2)\n",
    "    cw2, ch2 = int(crop_width/2), int(crop_height/2) \n",
    "    crop_img = img[miC:\\Users\\LeonCullens\\Repos\\ADAPT\\docker-compose.ymld_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture = cv2.VideoCapture('data/videos/17.mp4')\n",
    "# _, frame = capture.read()\n",
    "# H, W = frame.shape[:2]\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "# out = cv2.VideoWriter('data/videos/processed/17.mp4', fourcc, 30, (W,H))\n",
    "\n",
    "# COLOR1 = [255, 0, 0]\n",
    "# COLOR2 = [0, 0, 255]\n",
    "# # success, frame = capture.read()\n",
    "\n",
    "# model.to('cuda')\n",
    "# model.eval()\n",
    "\n",
    "# while capture.isOpened():\n",
    "#     start_time = time()\n",
    "#     _, frame = capture.read()\n",
    "#     image = frame[...,::-1]\n",
    "#     h, w = image.shape[:2]\n",
    "#     read_cam_time = time()\n",
    "\n",
    "#     X, pad_up, pad_left, h_new, w_new = utils.preprocessing(image, 1088, pad_value=0)\n",
    "#     preproc_time = time()\n",
    "#     with torch.no_grad():\n",
    "#         mask = model(X.cuda())\n",
    "#         mask = mask[..., pad_up: pad_up+h_new, pad_left: pad_left+w_new]\n",
    "#         mask = F.interpolate(mask, size=(h,w), mode='bilinear', align_corners=True)\n",
    "#         mask = F.softmax(mask, dim=1)\n",
    "#         mask = mask[0,0,...].cpu().numpy()\n",
    "#     predict_time = time()\n",
    "        \n",
    "#     image_alpha = utils.draw_transperency(image, mask, COLOR1, COLOR2)\n",
    "#     draw_time = time()\n",
    "    \n",
    "#     read = read_cam_time-start_time\n",
    "#     preproc = preproc_time-read_cam_time\n",
    "#     pred = predict_time-preproc_time\n",
    "#     draw = draw_time-predict_time\n",
    "#     total = read + preproc + pred + draw\n",
    "#     fps = 1 / total\n",
    "#     print(\"read: %.3f [s]; preproc: %.3f [s]; pred: %.3f [s]; draw: %.3f [s]; total: %.3f [s]; fps: %.2f [Hz]\" % \n",
    "#         (read, preproc, pred, draw, total, fps))\n",
    "    \n",
    "#     out.write(image_alpha[..., ::-1])\n",
    "#     cv2.imshow('webcam', image_alpha[..., ::-1])\n",
    "\n",
    "# capture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
